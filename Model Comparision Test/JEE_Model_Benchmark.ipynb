{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† JEE Question Generation Model Benchmark\n",
    "Compare multiple LLMs (GPT, Mistral, etc.) for generating JEE-style questions and evaluate their quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# !pip install openai transformers pandas matplotlib sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sympy import sympify, simplify\n",
    "from openai import OpenAI\n",
    "from transformers import pipeline\n",
    "\n",
    "# Setup API client (if using OpenAI)\n",
    "# os.environ['OPENAI_API_KEY'] = 'your_api_key_here'\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Load Small Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"subject\": \"Physics\", \"topic\": \"Kinematics\", \"difficulty\": \"Medium\", \"question\": \"A car accelerates from rest at 2 m/s¬≤. What is its velocity after 5 seconds?\", \"options\": [\"5 m/s\", \"10 m/s\", \"15 m/s\", \"20 m/s\"], \"correct_answer\": \"10 m/s\", \"solution\": \"v = u + at = 0 + 2√ó5 = 10 m/s\"},\n",
    "    {\"subject\": \"Maths\", \"topic\": \"Quadratic Equations\", \"difficulty\": \"Easy\", \"question\": \"Find the roots of the equation x¬≤ - 5x + 6 = 0.\", \"options\": [\"1,6\", \"2,3\", \"3,5\", \"1,5\"], \"correct_answer\": \"2,3\", \"solution\": \"Roots are (x-2)(x-3)=0\"},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Define Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question, solution):\n",
    "    return f'''\n",
    "You are a question generator for JEE exams.\n",
    "Generate ONE new JEE-style question that tests the same concept as below.\n",
    "Rephrase or change the numbers/context but keep difficulty same.\n",
    "Also generate 4 options and a correct answer with explanation.\n",
    "\n",
    "Question: {question}\n",
    "Solution: {solution}\n",
    "\n",
    "Output JSON with keys: question, options, correct_answer, explanation.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Model Query Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_gpt(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.8\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def generate_with_hf(model_name, prompt):\n",
    "    generator = pipeline(\"text-generation\", model=model_name, max_new_tokens=500)\n",
    "    result = generator(prompt, do_sample=True, temperature=0.8)\n",
    "    return result[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Generate Variants from Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"GPT-4o-mini\": lambda p: generate_with_gpt(p),\n",
    "    # \"Mistral-7B\": lambda p: generate_with_hf(\"mistralai/Mistral-7B-Instruct-v0.2\", p),\n",
    "}\n",
    "\n",
    "results = []\n",
    "for _, row in df.iterrows():\n",
    "    prompt = build_prompt(row['question'], row['solution'])\n",
    "    for model_name, func in models.items():\n",
    "        print(f\"\\nüîπ Generating with {model_name} for topic: {row['topic']}\")\n",
    "        output = func(prompt)\n",
    "        results.append({\n",
    "            'model': model_name,\n",
    "            'subject': row['subject'],\n",
    "            'topic': row['topic'],\n",
    "            'base_question': row['question'],\n",
    "            'generated_output': output\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Validation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_math_expression(expr1, expr2):\n",
    "    try:\n",
    "        return simplify(sympify(expr1) - sympify(expr2)) == 0\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Example usage:\n",
    "# validate_math_expression('2*5', '10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ Human Evaluation Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = results_df.copy()\n",
    "eval_df['conceptual_accuracy'] = None\n",
    "eval_df['clarity'] = None\n",
    "eval_df['creativity'] = None\n",
    "eval_df['answer_validity'] = None\n",
    "eval_df['formatting'] = None\n",
    "eval_df.to_csv('evaluation_template.csv', index=False)\n",
    "print('üìÑ Saved evaluation_template.csv ‚Äî fill scores (1‚Äì5) manually.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualization and Final Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored = pd.read_csv('evaluation_template.csv')\n",
    "avg_scores = scored.groupby('model')[['conceptual_accuracy', 'clarity', 'creativity', 'answer_validity', 'formatting']].mean()\n",
    "avg_scores.plot(kind='bar', figsize=(10,5), title='Model Comparison (Mean Scores)')\n",
    "plt.show()\n",
    "\n",
    "final_choice = avg_scores.mean(axis=1).idxmax()\n",
    "print(f'üèÜ Best Performing Model: {final_choice}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
